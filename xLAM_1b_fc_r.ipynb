{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PBICF/PH7/blob/master/xLAM_1b_fc_r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers json torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsEXaMW4iQ3P",
        "outputId": "516be78e-e45d-497f-9947-86598f1d12c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"Salesforce/xLAM-1b-fc-r\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka9EYsM6iQ3T",
        "outputId": "404c7f6a-7d7c-4298-c9b6-f62913abf7d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'user', 'content': 'Who are you?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"\\nI am an AI Programmed by IBM. I can assist with questions and tasks related to computer science, programming, and technology. I don't have personal identities, so I can't identify you as an individual.\"}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please use our provided instruction prompt for best performance\n",
        "task_instruction = \"\"\"\n",
        "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
        "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
        "If none of the functions can be used, point it out and refuse to answer.\n",
        "If the given question lacks the parameters required by the function, also point it out.\n",
        "\"\"\".strip()\n",
        "\n",
        "format_instruction = \"\"\"\n",
        "The output MUST strictly adhere to the following JSON format, and NO other text MUST be included.\n",
        "The example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make tool_calls an empty list '[]'.\n",
        "```\n",
        "{\n",
        "    \"tool_calls\": [\n",
        "    {\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}},\n",
        "    ... (more tool calls as required)\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\"\"\".strip()\n",
        "\n",
        "# Define the input query and available tools\n",
        "query = \"What's the weather like in chennai in clcius?\"\n",
        "\n",
        "get_weather_api = {\n",
        "    \"name\": \"get_weather\",\n",
        "    \"description\": \"Get the current weather for a location\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city and state, e.g. San Francisco, New York\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "                \"type\": \"string\",\n",
        "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                \"description\": \"The unit of temperature to return\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"location\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "search_api = {\n",
        "    \"name\": \"search\",\n",
        "    \"description\": \"Search for information on the internet\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The search query, e.g. 'latest news on AI'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"query\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "openai_format_tools = [get_weather_api, search_api]"
      ],
      "metadata": {
        "id": "88mDXT5Yk0Zp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Helper function to convert openai format tools to our more concise xLAM format\n",
        "def convert_to_xlam_tool(tools):\n",
        "    ''''''\n",
        "    if isinstance(tools, dict):\n",
        "        return {\n",
        "            \"name\": tools[\"name\"],\n",
        "            \"description\": tools[\"description\"],\n",
        "            \"parameters\": {k: v for k, v in tools[\"parameters\"].get(\"properties\", {}).items()}\n",
        "        }\n",
        "    elif isinstance(tools, list):\n",
        "        return [convert_to_xlam_tool(tool) for tool in tools]\n",
        "    else:\n",
        "        return tools\n",
        "\n",
        "# Helper function to build the input prompt for our model\n",
        "def build_prompt(task_instruction: str, format_instruction: str, tools: list, query: str):\n",
        "    prompt = f\"[BEGIN OF TASK INSTRUCTION]\\n{task_instruction}\\n[END OF TASK INSTRUCTION]\\n\\n\"\n",
        "    prompt += f\"[BEGIN OF AVAILABLE TOOLS]\\n{json.dumps(xlam_format_tools)}\\n[END OF AVAILABLE TOOLS]\\n\\n\"\n",
        "    prompt += f\"[BEGIN OF FORMAT INSTRUCTION]\\n{format_instruction}\\n[END OF FORMAT INSTRUCTION]\\n\\n\"\n",
        "    prompt += f\"[BEGIN OF QUERY]\\n{query}\\n[END OF QUERY]\\n\\n\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "6kXwL_cYk4LB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the input and start the inference\n",
        "xlam_format_tools = convert_to_xlam_tool(openai_format_tools)\n",
        "content = build_prompt(task_instruction, format_instruction, xlam_format_tools, query)"
      ],
      "metadata": {
        "id": "wJX2dXK7lB7I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b81bce5"
      },
      "source": [
        "def get_weather(location: str, unit: str = \"celsius\") -> str:\n",
        "    \"\"\"Simulates fetching current weather for a given location and unit.\"\"\"\n",
        "    print(f\"Simulating weather API call for {location} in {unit}.\")\n",
        "\n",
        "    # In a real application, you would make an API call here, e.g.:\n",
        "    import requests\n",
        "    api_key = \"98a0d31c429c49a8ae541644251112\"\n",
        "    url = f\"https://api.weatherapi.com/v1/current.json?key={api_key}&q={location}\"\n",
        "    response = requests.get(url).json()\n",
        "\n",
        "    # Process response to extract temperature in the desired unit\n",
        "    if unit == \"fahrenheit\":\n",
        "        temperature = response[\"current\"][\"temp_f\"]\n",
        "    else: # default to celsius\n",
        "        temperature = response[\"current\"][\"temp_c\"]\n",
        "    return f\"The current temperature in {location} is {temperature}°{unit[0].upper()}.\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e500c3a3"
      },
      "source": [
        "import json\n",
        "\n",
        "def handle_tool_calls(model_output):\n",
        "    try:\n",
        "        # Assuming model_output is a string like: {\"tool_calls\": [...]}\n",
        "        parsed_output = json.loads(model_output)\n",
        "        if \"tool_calls\" in parsed_output and isinstance(parsed_output[\"tool_calls\"], list):\n",
        "            for tool_call in parsed_output[\"tool_calls\"]:\n",
        "                tool_name = tool_call.get(\"name\")\n",
        "                tool_args = tool_call.get(\"arguments\", {})\n",
        "\n",
        "                if tool_name == \"get_weather\":\n",
        "                    print(f\"Calling get_weather with arguments: {tool_args}\")\n",
        "                    # Call the actual get_weather function\n",
        "                    result = get_weather(location=tool_args.get(\"location\"), unit=tool_args.get(\"unit\"))\n",
        "                    print(f\"Weather result: {result}\")\n",
        "                elif tool_name == \"search\":\n",
        "                    print(f\"Calling search with arguments: {tool_args}\")\n",
        "                    # Implement search function call here\n",
        "                    print(\"Search function is not implemented yet.\")\n",
        "                else:\n",
        "                    print(f\"Unknown tool: {tool_name}\")\n",
        "        else:\n",
        "            print(\"No valid 'tool_calls' found in the model output.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Invalid JSON output from model.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while handling tool calls: {e}\")\n",
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/xLAM-1b-fc-r\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/xLAM-1b-fc-r\")\n",
        "\n",
        "messages=[\n",
        "    { 'role': 'user', 'content': content}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# tokenizer.eos_token_id is the id of <|EOT|> token\n",
        "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
        "decoded_output = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "print(decoded_output)\n",
        "handle_tool_calls(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLm751ssiQ3U",
        "outputId": "c82be0d7-4370-439f-d414-d5a0b559c141"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{\"tool_calls\": [{\"name\": \"get_weather\", \"arguments\": {\"location\": \"chennai\", \"unit\": \"celsius\"}}]}\n",
            "Calling get_weather with arguments: {'location': 'chennai', 'unit': 'celsius'}\n",
            "Simulating weather API call for chennai in celsius.\n",
            "Weather result: The current temperature in chennai is 27.2°C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGlzpyffmxBp"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}